<!doctype html>
<html lang="en">

    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

        <link href="/static/css/bootstrap.min.css" rel="stylesheet" media="screen">
        <link href="/static/css/bootstrap-responsive.min.css" rel="stylesheet">
        <script src="http://code.jquery.com/jquery-latest.js"></script>
        <script src="/static/js/bootstrap.min.js"></script>
        <script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
        </script>
        <script type="text/javascript" async
                src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
        </script>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">


        <style> 
            .flex-container {
                text-align: center;
                list-style: none;
                display: flex;
                flex-flow: row wrap;
                align-items: center;
                justify-content: center;
            }
            .flex-figure-item {
                padding: 5px;
                margin-top: 10px;
                line-height: 10px;
                font-weight: bold;
                font-size: 1em;

                display: flex;           /* NEW */
                flex-direction: column;  /* NEW */
                width: 900px;            /* NEW */
            }
            figcaption {
                color: black;
            }
            body { padding-top: 40px;
                background-color : lightgrey;} 
            label.text{
                font-size: 20px;}
            .greybg {background-color :  #e7e7e9}
            div.form {background-color :  #f7f7f9;
                border: solid #c7c7c9;}
            .centered
            {
                text-align:center;
            }
            .right
            {
                text-align:right;
            }
        </style>
        <script>

            function to_disable(val, ids)
            {for (i=0; i < ids.length; i++){
                if(val == 'remote'){
                    console.log('disabling');
                    document.getElementsByName(ids[i])[0].disabled=false;
                }else{
                    console.log('enabling');
                    document.getElementsByName(ids[i])[0].disabled=true;
                }
            }
            }
        </script>
        <script> 
            $(function(){
                $("#navbar").load("static/snippets/navbar.html"); 
            });
        </script>
        <meta charset="utf-8">
        <title>Deep Learning</title>
        <link href="css/bootstrap.min.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
        <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
        <!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->

    </head>

    <body style = "background-color : #ffffff;">
        <div class="container" id = "navbar"></div>

        <div class="jumbotron">

            <div class = "flex-container" >
                <div class = "centered">

                    <figure class = "flex-figure-item">
                        <img src = "static/pics/flow_chart.jpg" alt = "can't find!" style = "text-align:center" class = "centered">
                        <figcaption class = "centered"><h3 style = "text-align:justify">A policy (move-choosing) neural network is trained on a data set of human moves, then the policy network generates a data set through self-play. A value (position-evaluating) neural network then trains on this new data set by receiving reward signals whenever a game completes. The reward signal propagates to all positions that lead to the terminal state through temporal difference learning

                            $$ P_t \rightarrow \sum_{n=0}^{n=T} \lambda^n (1-\lambda) V_{t+n+1}$$

                            where $n = T$ is the terminal state and $V_{t+T+1} = r$ is the reward signal. The parameter $\lambda$ is chosen to balance how deeply a reward signal propagates. A value of $\lambda=1$ (Monte-Carlo learning) indicates that only the end-state $V_{t+T+1} = r$ is considered when updating $V_t$, while $\lambda = 0$ indicates that only the value network's evaluation of the very next move $V_{t+1}$ is considered. The value network is then trained on this new data by minimizing its mean squared error with respect to these new targets generated through reinforcement learning, and the policy network is trained by minimizing the log-loss from value network predictions, completing the training cycle.</h3></figcaption>

                    </figure>
                </div>
            </div>


        </div>
    </body>
</html>

